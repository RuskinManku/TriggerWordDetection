{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import IPython\n",
    "from td_utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"raw_data/activates/1.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"raw_data/negatives/3.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"raw_data/backgrounds/1.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 5511 # The number of time steps input to the model from the spectrogram\n",
    "n_freq = 101 # Number of frequencies input to the model at each time step of the spectrogram\n",
    "Ty = 1375 # The number of time steps in the output of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activates, negatives, backgrounds = load_raw_audio()\n",
    "print(\"background len: \" + str(len(backgrounds[0])))    # Should be 10,000, since it is a 10 sec clip\n",
    "print(\"activate[0] len: \" + str(len(activates[0])))     # Maybe around 1000, since an \"activate\" audio clip is usually around 1 sec (but varies a lot)\n",
    "print(\"activate[1] len: \" + str(len(activates[1])))     # Different \"activate\" clips can have different lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_time_segment(segment_ms):\n",
    "    \"\"\"\n",
    "    Gets a random time segment of duration segment_ms in a 10,000 ms audio clip.\n",
    "    \"\"\"\n",
    "    segment_start = np.random.randint(low=0, high=10000-segment_ms)  \n",
    "    segment_end = segment_start + segment_ms - 1\n",
    "    return (segment_start, segment_end)\n",
    "\n",
    "def is_overlapping(segment_time, previous_segments):\n",
    "    \"\"\"\n",
    "    Checks if the time of a segment overlaps with the times of existing segments.\n",
    "    \"\"\"\n",
    "    segment_start, segment_end = segment_time\n",
    "    overlap = False\n",
    "    for previous_start, previous_end in previous_segments:\n",
    "        if segment_start <= previous_end and segment_end >= previous_start:\n",
    "            overlap = True\n",
    "    return overlap\n",
    "\n",
    "def insert_audio_clip(background, audio_clip, previous_segments):\n",
    "    \"\"\"\n",
    "    Insert a new audio segment over the background noise at a random time step, ensuring that the \n",
    "    audio segment does not overlap with existing segments.\n",
    "    \"\"\"\n",
    "    segment_ms = len(audio_clip)\n",
    "    segment_time = get_random_time_segment(segment_ms)\n",
    "    while is_overlapping(segment_time, previous_segments):\n",
    "        segment_time = get_random_time_segment(segment_ms)\n",
    "\n",
    "    previous_segments.append(segment_time)\n",
    "    new_background = background.overlay(audio_clip, position = segment_time[0])\n",
    "    return new_background, segment_time\n",
    "\n",
    "def insert_ones(y, segment_end_ms):\n",
    "    \"\"\"\n",
    "    Update the label vector y.\n",
    "    \"\"\"\n",
    "    segment_end_y = int(segment_end_ms * Ty / 10000.0)\n",
    "    for i in range(segment_end_y + 1, segment_end_y + 51):\n",
    "        if i < Ty:\n",
    "            y[0, i] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_example(background, activates, negatives):\n",
    "    \"\"\"\n",
    "    Creates a training example with a given background, activates, and negatives.\n",
    "    \"\"\"\n",
    "    np.random.seed(18)\n",
    "    background = background - 20\n",
    "    y = np.zeros((1, Ty))\n",
    "    previous_segments = []\n",
    "    # Select 0-4 random \"activate\" audio clips from the entire list of \"activates\" recordings\n",
    "    number_of_activates = np.random.randint(0, 5)\n",
    "    random_indices = np.random.randint(len(activates), size=number_of_activates)\n",
    "    random_activates = [activates[i] for i in random_indices]\n",
    "    for random_activate in random_activates:\n",
    "        background, segment_time = insert_audio_clip(background, random_activate, previous_segments)\n",
    "        segment_start, segment_end = segment_time\n",
    "        y = insert_ones(y, segment_end_ms=segment_end)\n",
    "        \n",
    "    # Select 0-2 random negatives audio recordings from the entire list of \"negatives\" recordings\n",
    "    number_of_negatives = np.random.randint(0, 3)\n",
    "    random_indices = np.random.randint(len(negatives), size=number_of_negatives)\n",
    "    random_negatives = [negatives[i] for i in random_indices]\n",
    "    for random_negative in random_negatives:\n",
    "        background, _ = insert_audio_clip(background, random_negative, previous_segments)\n",
    "    background = match_target_amplitude(background, -20.0)\n",
    "    file_handle = background.export(\"train\" + \".wav\", format=\"wav\")\n",
    "    print(\"File (train.wav) was saved in your directory.\")\n",
    "    \n",
    "    # Get and plot spectrogram of the new recording (background with superposition of positive and negatives)\n",
    "    x = graph_spectrogram(\"train.wav\")\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = create_training_example(backgrounds[0], activates, negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"train.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = np.load(\"./Dev/X_dev.npy\")\n",
    "Y_dev = np.load(\"./Dev/Y_dev.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
    "from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_shape):\n",
    "    \"\"\"\n",
    "    Function creating the model's graph in Keras.\n",
    "    \"\"\"\n",
    "    X_input = Input(shape = input_shape)\n",
    "    #CONV layer\n",
    "    X = Conv1D(196, kernel_size=15, strides=4)(X_input)                             \n",
    "    X = BatchNormalization()(X)                             \n",
    "    X = Activation('relu')(X)                           \n",
    "    X = Dropout(0.8)(X)                         \n",
    "\n",
    "    # First GRU Layer\n",
    "    X = GRU(units = 128, return_sequences = True)(X) \n",
    "    X = Dropout(0.8)(X)                                \n",
    "    X = BatchNormalization()(X)                                \n",
    "    \n",
    "    # Second GRU Layer\n",
    "    X = GRU(units = 128, return_sequences = True)(X)   \n",
    "    X = Dropout(0.8)(X)                               \n",
    "    X = BatchNormalization()(X)                                 \n",
    "    X = Dropout(0.8)(X)                             \n",
    "    \n",
    "    # Time-distributed dense layer\n",
    "    X = TimeDistributed(Dense(1, activation = \"sigmoid\"))(X)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(input_shape = (Tx, n_freq))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./models/tr_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_dev, Y_dev)\n",
    "print(\"Dev set accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_triggerword(filename):\n",
    "    plt.subplot(2, 1, 1)\n",
    "    x = graph_spectrogram(filename)\n",
    "    x  = x.swapaxes(0,1)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    predictions = model.predict(x)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(predictions[0,:,0])\n",
    "    plt.ylabel('probability')\n",
    "    plt.show()\n",
    "    return predictions\n",
    "\n",
    "chime_file = \"audio_examples/chime.wav\"\n",
    "def chime_on_activate(filename, predictions, threshold):\n",
    "    audio_clip = AudioSegment.from_wav(filename)\n",
    "    chime = AudioSegment.from_wav(chime_file)\n",
    "    Ty = predictions.shape[1]\n",
    "    consecutive_timesteps = 0\n",
    "    for i in range(Ty):\n",
    "        consecutive_timesteps += 1\n",
    "        # Step 4: If prediction is higher than the threshold and more than 75 consecutive output steps have passed\n",
    "        if predictions[0,i,0] > threshold and consecutive_timesteps > 75:\n",
    "            audio_clip = audio_clip.overlay(chime, position = ((i / Ty) * audio_clip.duration_seconds)*1000)\n",
    "            consecutive_timesteps = 0\n",
    "        \n",
    "    audio_clip.export(\"chime_output.wav\", format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/dev/1.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/dev/2.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./raw_data/dev/1.wav\"\n",
    "prediction = detect_triggerword(filename)\n",
    "chime_on_activate(filename, prediction, 0.5)\n",
    "IPython.display.Audio(\"./chime_output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename  = \"./raw_data/dev/2.wav\"\n",
    "prediction = detect_triggerword(filename)\n",
    "chime_on_activate(filename, prediction, 0.5)\n",
    "IPython.display.Audio(\"./chime_output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TriggerWord",
   "language": "python",
   "name": "triggerword"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
